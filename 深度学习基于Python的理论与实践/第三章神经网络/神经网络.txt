用图来表示神经网络的话， 最左边是输入层， 最右边是输出层。
中间的一列称为中间层。

h(x)函数将输入信号的总和转换为输出总和。 这种函数是激活函数。

激活函数是连接感知机和神经网络的桥梁。

sigmoid函数 
              1
h(x) =  -------------
	  1 + exp(-x)
实现跃迁函数
def step_function():
	if x > 0 :
		return 1
	else:
		return 0
实现sigmoid函数
	def sigmoid(x):
		return 1 / (1 + np.exp(-x))
感知机中神经元之间流动的是0或1的二元信号， 而神经网络中流动的是连接的实数值信号。

神经网络的激活函数必须使用线性函数。 使用线性函数， 加深神经网络的层数就没有意义了。


ReLU函数:
	在输入大于0时， 直接输出该值
	在输入小等于0时， 输出0
实现RElU函数：
def relu(x):
	return np.maximum(0, x)


数组的维数可以通过 np.dim()获得

矩阵的乘积是通过左边矩阵的行和右边矩阵的列以对应元素的方式相乘后再求和而得到的。

前向： 输入到输出方向的传递处理
后向： 输出到输入方向的传递处理

机器学习：
	分类问题
	回归问题

softmax函数

	exp(Ak)
Yk = -------------
     for i in [1,n]exp(Ai) ++ //求和[1, n]

实现softmax函数
def softmax(a):
	c = np.max(a)
	exp_a = np.exp(a - c)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a

	return y

输出总和为1是softmax函数的一个重要性质


求解步骤：
	学习
	推理











